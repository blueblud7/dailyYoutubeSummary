import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from sqlalchemy.orm import Session
from sqlalchemy import and_, func
from app.models.database import Video, Analysis, Channel, Keyword, PersonInfluencer
from app.services.analysis_service import AnalysisService
from app.services.notification_service import NotificationService

class PersonalizedReportService:
    def __init__(self):
        self.analysis_service = AnalysisService()
        self.notification_service = NotificationService()
        self.logger = logging.getLogger(__name__)
    
    def generate_keyword_focused_report(self, db: Session, keyword: str, 
                                      days_back: int = 1) -> Dict:
        """íŠ¹ì • í‚¤ì›Œë“œì— ì§‘ì¤‘í•œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # í‚¤ì›Œë“œ ê´€ë ¨ ë¶„ì„ë“¤ ê°€ì ¸ì˜¤ê¸°
        keyword_obj = db.query(Keyword).filter(Keyword.keyword == keyword).first()
        if not keyword_obj:
            return {
                "keyword": keyword,
                "message": f"'{keyword}' í‚¤ì›Œë“œê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        analyses = db.query(Analysis).join(Video).filter(
            and_(
                Analysis.keyword_id == keyword_obj.id,
                Video.published_at >= start_date,
                Video.published_at <= end_date
            )
        ).all()
        
        if not analyses:
            return {
                "keyword": keyword,
                "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
                "message": f"í•´ë‹¹ ê¸°ê°„ì— '{keyword}' ê´€ë ¨ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ë¶„ì„ ë°ì´í„° ê°€ê³µ
        analysis_data = []
        for analysis in analyses:
            video = db.query(Video).filter(Video.video_id == analysis.video_id).first()
            channel = db.query(Channel).filter(Channel.channel_id == video.channel_id).first()
            
            analysis_data.append({
                'summary': analysis.summary,
                'sentiment_score': analysis.sentiment_score,
                'importance_score': analysis.importance_score,
                'key_insights': json.loads(analysis.key_insights) if analysis.key_insights else [],
                'mentioned_entities': json.loads(analysis.mentioned_entities) if analysis.mentioned_entities else [],
                'video_title': video.title,
                'channel_name': channel.channel_name if channel else 'Unknown',
                'published_at': video.published_at,
                'video_url': video.video_url,
                'view_count': video.view_count
            })
        
        # í‚¤ì›Œë“œë³„ íŠ¸ë Œë“œ ë¶„ì„
        keyword_trend = self.analysis_service.generate_trend_analysis(
            analysis_data, [keyword], f"ìµœê·¼ {days_back}ì¼"
        )
        
        # ì±„ë„ë³„ ê´€ì  ì •ë¦¬
        channel_perspectives = {}
        for data in analysis_data:
            channel_name = data['channel_name']
            if channel_name not in channel_perspectives:
                channel_perspectives[channel_name] = []
            channel_perspectives[channel_name].append(data)
        
        # í†µê³„ ì •ë³´
        statistics = {
            "total_analyses": len(analysis_data),
            "total_channels": len(channel_perspectives),
            "avg_sentiment": sum(d['sentiment_score'] for d in analysis_data) / len(analysis_data),
            "avg_importance": sum(d['importance_score'] for d in analysis_data) / len(analysis_data),
            "sentiment_distribution": {
                "positive": len([d for d in analysis_data if d['sentiment_score'] > 0.1]),
                "neutral": len([d for d in analysis_data if -0.1 <= d['sentiment_score'] <= 0.1]),
                "negative": len([d for d in analysis_data if d['sentiment_score'] < -0.1])
            }
        }
        
        return {
            "keyword": keyword,
            "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
            "keyword_trend": keyword_trend,
            "channel_perspectives": channel_perspectives,
            "top_videos": sorted(analysis_data, key=lambda x: x['importance_score'], reverse=True)[:5],
            "statistics": statistics
        }
    
    def generate_influencer_focused_report(self, db: Session, influencer_name: str, 
                                         days_back: int = 7) -> Dict:
        """íŠ¹ì • ì¸í”Œë£¨ì–¸ì„œì— ì§‘ì¤‘í•œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # ì¸í”Œë£¨ì–¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        influencer = db.query(PersonInfluencer).filter(
            PersonInfluencer.name == influencer_name
        ).first()
        
        if not influencer:
            return {
                "influencer": influencer_name,
                "message": f"'{influencer_name}' ì¸í”Œë£¨ì–¸ì„œê°€ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # í•´ë‹¹ ì¸í”Œë£¨ì–¸ì„œê°€ ì–¸ê¸‰ëœ ë¶„ì„ë“¤ ì°¾ê¸°
        analyses = db.query(Analysis).join(Video).filter(
            and_(
                Video.published_at >= start_date,
                Video.published_at <= end_date,
                Analysis.mentioned_entities.like(f'%{influencer_name}%')
            )
        ).all()
        
        if not analyses:
            return {
                "influencer": influencer_name,
                "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
                "message": f"í•´ë‹¹ ê¸°ê°„ì— '{influencer_name}' ê´€ë ¨ ì–¸ê¸‰ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ë¶„ì„ ë°ì´í„° ê°€ê³µ
        analysis_data = []
        for analysis in analyses:
            video = db.query(Video).filter(Video.video_id == analysis.video_id).first()
            channel = db.query(Channel).filter(Channel.channel_id == video.channel_id).first()
            
            analysis_data.append({
                'summary': analysis.summary,
                'sentiment_score': analysis.sentiment_score,
                'importance_score': analysis.importance_score,
                'key_insights': json.loads(analysis.key_insights) if analysis.key_insights else [],
                'mentioned_entities': json.loads(analysis.mentioned_entities) if analysis.mentioned_entities else [],
                'video_title': video.title,
                'channel_name': channel.channel_name if channel else 'Unknown',
                'published_at': video.published_at,
                'video_url': video.video_url
            })
        
        # ì¸í”Œë£¨ì–¸ì„œ ì–¸ê¸‰ ë¶„ì„
        mention_analysis = self._analyze_influencer_mentions(analysis_data, influencer_name)
        
        return {
            "influencer": influencer_name,
            "specialty": influencer.specialty if influencer else "ì „ë¬¸ ë¶„ì•¼ ë¯¸ìƒ",
            "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
            "mention_analysis": mention_analysis,
            "related_videos": sorted(analysis_data, key=lambda x: x['importance_score'], reverse=True)[:10],
            "statistics": {
                "total_mentions": len(analysis_data),
                "avg_sentiment": sum(d['sentiment_score'] for d in analysis_data) / len(analysis_data),
                "channels_mentioned": len(set(d['channel_name'] for d in analysis_data))
            }
        }
    
    def generate_channel_focused_report(self, db: Session, channel_name: str, 
                                      days_back: int = 7) -> Dict:
        """íŠ¹ì • ì±„ë„ì— ì§‘ì¤‘í•œ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        # ì±„ë„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        channel = db.query(Channel).filter(Channel.channel_name == channel_name).first()
        
        if not channel:
            return {
                "channel": channel_name,
                "message": f"'{channel_name}' ì±„ë„ì´ ë“±ë¡ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }
        
        # í•´ë‹¹ ì±„ë„ì˜ ë¹„ë””ì˜¤ë“¤ê³¼ ë¶„ì„ ê°€ì ¸ì˜¤ê¸°
        videos = db.query(Video).filter(
            and_(
                Video.channel_id == channel.channel_id,
                Video.published_at >= start_date,
                Video.published_at <= end_date
            )
        ).all()
        
        if not videos:
            return {
                "channel": channel_name,
                "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
                "message": f"í•´ë‹¹ ê¸°ê°„ì— '{channel_name}' ì±„ë„ì˜ ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ë¹„ë””ì˜¤ë³„ ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘
        video_analyses = []
        for video in videos:
            analyses = db.query(Analysis).filter(Analysis.video_id == video.video_id).all()
            
            for analysis in analyses:
                keyword = db.query(Keyword).filter(Keyword.id == analysis.keyword_id).first()
                
                video_analyses.append({
                    'video_title': video.title,
                    'published_at': video.published_at,
                    'view_count': video.view_count,
                    'like_count': video.like_count,
                    'video_url': video.video_url,
                    'summary': analysis.summary,
                    'sentiment_score': analysis.sentiment_score,
                    'importance_score': analysis.importance_score,
                    'key_insights': json.loads(analysis.key_insights) if analysis.key_insights else [],
                    'mentioned_entities': json.loads(analysis.mentioned_entities) if analysis.mentioned_entities else [],
                    'keyword': keyword.keyword if keyword else 'Unknown'
                })
        
        if not video_analyses:
            return {
                "channel": channel_name,
                "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
                "message": f"í•´ë‹¹ ê¸°ê°„ì— '{channel_name}' ì±„ë„ì˜ ë¶„ì„ì´ ì—†ìŠµë‹ˆë‹¤."
            }
        
        # ì±„ë„ íŠ¸ë Œë“œ ë¶„ì„
        channel_trend = self.analysis_service.generate_trend_analysis(
            video_analyses, [], f"ìµœê·¼ {days_back}ì¼"
        )
        
        # ì±„ë„ í†µê³„
        statistics = {
            "total_videos": len(videos),
            "total_analyses": len(video_analyses),
            "avg_views": sum(v.view_count for v in videos) / len(videos),
            "avg_sentiment": sum(va['sentiment_score'] for va in video_analyses) / len(video_analyses),
            "most_mentioned_entities": self._get_top_entities(video_analyses, 10),
            "content_keywords": list(set(va['keyword'] for va in video_analyses))
        }
        
        return {
            "channel": channel_name,
            "subscriber_count": channel.subscriber_count,
            "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
            "channel_trend": channel_trend,
            "top_videos": sorted(video_analyses, key=lambda x: x['importance_score'], reverse=True)[:5],
            "recent_videos": sorted(video_analyses, key=lambda x: x['published_at'], reverse=True)[:10],
            "statistics": statistics
        }
    
    def generate_multi_dimension_report(self, db: Session, keywords: List[str] = None,
                                      channels: List[str] = None, 
                                      influencers: List[str] = None,
                                      days_back: int = 7) -> Dict:
        """ë‹¤ì°¨ì› ê°œì¸í™” ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_back)
        
        report_sections = {}
        
        # í‚¤ì›Œë“œë³„ ë¶„ì„
        if keywords:
            keyword_reports = {}
            for keyword in keywords:
                keyword_reports[keyword] = self.generate_keyword_focused_report(
                    db, keyword, days_back
                )
            report_sections['keywords'] = keyword_reports
        
        # ì±„ë„ë³„ ë¶„ì„
        if channels:
            channel_reports = {}
            for channel in channels:
                channel_reports[channel] = self.generate_channel_focused_report(
                    db, channel, days_back
                )
            report_sections['channels'] = channel_reports
        
        # ì¸í”Œë£¨ì–¸ì„œë³„ ë¶„ì„
        if influencers:
            influencer_reports = {}
            for influencer in influencers:
                influencer_reports[influencer] = self.generate_influencer_focused_report(
                    db, influencer, days_back
                )
            report_sections['influencers'] = influencer_reports
        
        # ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±
        overall_insights = self._generate_overall_insights(report_sections)
        
        return {
            "report_type": "multi_dimension",
            "period": f"{start_date.strftime('%Y-%m-%d')} ~ {end_date.strftime('%Y-%m-%d')}",
            "sections": report_sections,
            "overall_insights": overall_insights,
            "generated_at": datetime.now().isoformat()
        }
    
    def _analyze_influencer_mentions(self, analysis_data: List[Dict], 
                                   influencer_name: str) -> Dict:
        """ì¸í”Œë£¨ì–¸ì„œ ì–¸ê¸‰ ë¶„ì„"""
        
        mention_contexts = []
        sentiment_scores = []
        
        for data in analysis_data:
            # ì¸í”Œë£¨ì–¸ì„œê°€ ì–¸ê¸‰ëœ ë§¥ë½ ë¶„ì„
            for insight in data['key_insights']:
                if influencer_name in insight:
                    mention_contexts.append({
                        'context': insight,
                        'sentiment': data['sentiment_score'],
                        'video_title': data['video_title'],
                        'channel': data['channel_name']
                    })
            
            sentiment_scores.append(data['sentiment_score'])
        
        avg_sentiment = sum(sentiment_scores) / len(sentiment_scores) if sentiment_scores else 0
        
        return {
            "mention_contexts": mention_contexts[:10],  # ìƒìœ„ 10ê°œë§Œ
            "avg_sentiment_when_mentioned": avg_sentiment,
            "sentiment_interpretation": self._get_sentiment_interpretation(avg_sentiment),
            "total_mentions": len(analysis_data)
        }
    
    def _get_top_entities(self, analyses: List[Dict], limit: int = 10) -> List[Dict]:
        """ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ ì—”í‹°í‹°ë“¤ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
        
        entity_counts = {}
        for analysis in analyses:
            for entity in analysis.get('mentioned_entities', []):
                entity_counts[entity] = entity_counts.get(entity, 0) + 1
        
        sorted_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)
        return [{"entity": entity, "count": count} for entity, count in sorted_entities[:limit]]
    
    def _get_sentiment_interpretation(self, sentiment_score: float) -> str:
        """ê°ì • ì ìˆ˜ í•´ì„"""
        if sentiment_score > 0.3:
            return "ë§¤ìš° ê¸ì •ì "
        elif sentiment_score > 0.1:
            return "ê¸ì •ì "
        elif sentiment_score > -0.1:
            return "ì¤‘ë¦½ì "
        elif sentiment_score > -0.3:
            return "ë¶€ì •ì "
        else:
            return "ë§¤ìš° ë¶€ì •ì "
    
    def _generate_overall_insights(self, report_sections: Dict) -> Dict:
        """ì¢…í•© ì¸ì‚¬ì´íŠ¸ ìƒì„±"""
        
        insights = {
            "key_themes": [],
            "sentiment_summary": {},
            "trending_topics": [],
            "recommendations": []
        }
        
        # ê° ì„¹ì…˜ì—ì„œ ì£¼ìš” í…Œë§ˆ ì¶”ì¶œ
        all_themes = []
        all_sentiments = []
        
        for section_type, section_data in report_sections.items():
            for item_name, item_data in section_data.items():
                if 'statistics' in item_data:
                    if 'avg_sentiment' in item_data['statistics']:
                        all_sentiments.append(item_data['statistics']['avg_sentiment'])
                
                # ê° ì„¹ì…˜ì˜ í‚¤ ì¸ì‚¬ì´íŠ¸ ìˆ˜ì§‘
                if section_type == 'keywords' and 'keyword_trend' in item_data:
                    themes = item_data['keyword_trend'].get('key_themes', [])
                    all_themes.extend(themes)
                elif section_type == 'channels' and 'channel_trend' in item_data:
                    themes = item_data['channel_trend'].get('key_themes', [])
                    all_themes.extend(themes)
        
        # ê°€ì¥ ìì£¼ ì–¸ê¸‰ë˜ëŠ” í…Œë§ˆë“¤
        theme_counts = {}
        for theme in all_themes:
            theme_counts[theme] = theme_counts.get(theme, 0) + 1
        
        insights['key_themes'] = sorted(theme_counts.items(), key=lambda x: x[1], reverse=True)[:5]
        
        # ì „ì²´ ê°ì • ìš”ì•½
        if all_sentiments:
            avg_sentiment = sum(all_sentiments) / len(all_sentiments)
            insights['sentiment_summary'] = {
                "overall_sentiment": avg_sentiment,
                "interpretation": self._get_sentiment_interpretation(avg_sentiment)
            }
        
        return insights
    
    def send_personalized_notification(self, report_data: Dict, notification_type: str = "email") -> bool:
        """ê°œì¸í™”ëœ ë¦¬í¬íŠ¸ ì•Œë¦¼ ë°œì†¡"""
        
        try:
            if notification_type == "email":
                subject, html_body = self._format_personalized_email(report_data)
                return self.notification_service.send_email(subject, html_body)
            elif notification_type == "slack":
                message = self._format_personalized_slack_message(report_data)
                return self.notification_service.send_slack_message(message)
            else:
                self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì•Œë¦¼ íƒ€ì…: {notification_type}")
                return False
                
        except Exception as e:
            self.logger.error(f"ê°œì¸í™”ëœ ì•Œë¦¼ ë°œì†¡ ì‹¤íŒ¨: {e}")
            return False
    
    def _format_personalized_email(self, report_data: Dict) -> tuple:
        """ê°œì¸í™”ëœ ë¦¬í¬íŠ¸ ì´ë©”ì¼ í¬ë§·íŒ…"""
        
        report_type = report_data.get('report_type', 'personalized')
        period = report_data.get('period', 'ìµœê·¼')
        
        if 'keyword' in report_data:
            # í‚¤ì›Œë“œ ë¦¬í¬íŠ¸
            keyword = report_data['keyword']
            subject = f"ğŸ” '{keyword}' í‚¤ì›Œë“œ ë¶„ì„ ë¦¬í¬íŠ¸ ({period})"
        elif 'channel' in report_data:
            # ì±„ë„ ë¦¬í¬íŠ¸
            channel = report_data['channel']
            subject = f"ğŸ“º '{channel}' ì±„ë„ ë¶„ì„ ë¦¬í¬íŠ¸ ({period})"
        elif 'influencer' in report_data:
            # ì¸í”Œë£¨ì–¸ì„œ ë¦¬í¬íŠ¸
            influencer = report_data['influencer']
            subject = f"ğŸ‘¤ '{influencer}' ì¸í”Œë£¨ì–¸ì„œ ì–¸ê¸‰ ë¶„ì„ ({period})"
        else:
            # ë‹¤ì°¨ì› ë¦¬í¬íŠ¸
            subject = f"ğŸ“Š ê°œì¸í™”ëœ íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë¦¬í¬íŠ¸ ({period})"
        
        # ê°„ë‹¨í•œ HTML ë³¸ë¬¸ (ì‹¤ì œë¡œëŠ” ë” ìƒì„¸í•˜ê²Œ êµ¬í˜„)
        html_body = f"""
        <html>
        <body>
        <h1>{subject}</h1>
        <p>ê°œì¸í™”ëœ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.</p>
        <p><strong>ë¦¬í¬íŠ¸ ìƒì„± ì‹œê°„:</strong> {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </body>
        </html>
        """
        
        return subject, html_body
    
    def _format_personalized_slack_message(self, report_data: Dict) -> str:
        """ê°œì¸í™”ëœ ë¦¬í¬íŠ¸ ìŠ¬ë™ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        
        if 'keyword' in report_data:
            keyword = report_data['keyword']
            stats = report_data.get('statistics', {})
            return f"ğŸ” *{keyword} í‚¤ì›Œë“œ ë¶„ì„ ì™„ë£Œ*\nâ€¢ ë¶„ì„ ìˆ˜: {stats.get('total_analyses', 0)}ê°œ\nâ€¢ í‰ê·  ê°ì •: {stats.get('avg_sentiment', 0):.2f}"
        
        return "ğŸ“Š ê°œì¸í™”ëœ ë¦¬í¬íŠ¸ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤." 