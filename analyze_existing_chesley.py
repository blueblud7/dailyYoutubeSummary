#!/usr/bin/env python3

from app.models.database import SessionLocal, Video, Analysis, Transcript
from datetime import datetime, timedelta
import json

def analyze_existing_chesley():
    """ë°ì´í„°ë² ì´ìŠ¤ì— ìˆëŠ” ì²´ìŠ¬ë¦¬TV ë°ì´í„°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    print("ğŸŒ… ì²´ìŠ¬ë¦¬TV ê¸°ì¡´ ë°ì´í„° ë¶„ì„")
    print("="*50)
    
    db = SessionLocal()
    
    try:
        # ì²´ìŠ¬ë¦¬TV ì±„ë„ ID
        chesley_channel_id = "UCXST0Hq6CAmG0dmo3jgrlEw"
        
        # ìµœê·¼ 7ì¼ ë°ì´í„° ì¡°íšŒ
        recent_date = datetime.now() - timedelta(days=7)
        videos = db.query(Video).filter(
            Video.channel_id == chesley_channel_id,
            Video.published_at >= recent_date
        ).order_by(Video.published_at.desc()).all()
        
        print(f"ğŸ“º ìµœê·¼ 7ì¼ ì²´ìŠ¬ë¦¬TV ë¹„ë””ì˜¤: {len(videos)}ê°œ")
        
        # ëª¨ë‹ë¸Œë¦¬í”„ë§Œ í•„í„°ë§
        morning_brief_videos = []
        for video in videos:
            title_lower = video.title.lower()
            if any(keyword in title_lower for keyword in ['ëª¨ë‹ë¸Œë¦¬í”„', 'morning brief', 'ëª¨ë‹', 'morning']):
                morning_brief_videos.append(video)
        
        print(f"ğŸŒ… ëª¨ë‹ë¸Œë¦¬í”„ ë¹„ë””ì˜¤: {len(morning_brief_videos)}ê°œ")
        
        # ëª¨ë‹ë¸Œë¦¬í”„ê°€ ì—†ìœ¼ë©´ ì „ì²´ ë¹„ë””ì˜¤ ì¤‘ ìµœì‹  5ê°œ ë¶„ì„
        if not morning_brief_videos:
            print("ğŸ“º ëª¨ë‹ë¸Œë¦¬í”„ê°€ ì—†ì–´ ìµœì‹  ë¹„ë””ì˜¤ë“¤ì„ ë¶„ì„í•©ë‹ˆë‹¤:")
            analysis_videos = videos[:5]
        else:
            analysis_videos = morning_brief_videos
        
        # ë¹„ë””ì˜¤ë³„ ìƒì„¸ ì •ë³´ ìˆ˜ì§‘
        analysis_results = []
        for video in analysis_videos:
            # ë¶„ì„ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
            analysis = db.query(Analysis).filter(
                Analysis.video_id == video.video_id
            ).first()
            
            # ìë§‰ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°  
            transcript = db.query(Transcript).filter(
                Transcript.video_id == video.video_id
            ).first()
            
            video_info = {
                'video_title': video.title,
                'published_at': video.published_at,
                'view_count': video.view_count,
                'video_url': video.video_url,
                'has_transcript': transcript is not None,
                'transcript_language': transcript.language if transcript else None,
                'transcript_auto': transcript.is_auto_generated if transcript else None,
                'has_analysis': analysis is not None
            }
            
            if analysis:
                video_info.update({
                    'summary': analysis.summary,
                    'sentiment_score': analysis.sentiment_score,
                    'importance_score': analysis.importance_score,
                    'key_insights': json.loads(analysis.key_insights) if analysis.key_insights else [],
                    'mentioned_entities': json.loads(analysis.mentioned_entities) if analysis.mentioned_entities else []
                })
            
            analysis_results.append(video_info)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ¯ ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        print(f"- ë¶„ì„ ê¸°ê°„: {recent_date.strftime('%Y-%m-%d')} ~ {datetime.now().strftime('%Y-%m-%d')}")
        print(f"- ì „ì²´ ë¹„ë””ì˜¤: {len(videos)}ê°œ")
        print(f"- ë¶„ì„ ëŒ€ìƒ: {len(analysis_videos)}ê°œ")
        
        # ìë§‰/ë¶„ì„ í†µê³„
        transcript_count = sum(1 for v in analysis_results if v['has_transcript'])
        analysis_count = sum(1 for v in analysis_results if v['has_analysis'])
        
        print(f"- ìë§‰ ë³´ìœ : {transcript_count}/{len(analysis_results)}ê°œ")
        print(f"- ë¶„ì„ ì™„ë£Œ: {analysis_count}/{len(analysis_results)}ê°œ")
        
        if analysis_count > 0:
            avg_sentiment = sum(v.get('sentiment_score', 0) for v in analysis_results if v['has_analysis']) / analysis_count
            print(f"- í‰ê·  ê°ì • ì ìˆ˜: {avg_sentiment:.2f}")
        
        print(f"\nğŸ“‹ ìƒì„¸ ë¶„ì„ ê²°ê³¼:")
        print("="*60)
        
        for i, video in enumerate(analysis_results, 1):
            print(f"\n{i}. ğŸ“º {video['video_title']}")
            print(f"   ğŸ“… {video['published_at'].strftime('%Y-%m-%d %H:%M')}")
            print(f"   ğŸ‘€ ì¡°íšŒìˆ˜: {video['view_count']:,}")
            
            # ìë§‰ ì •ë³´
            if video['has_transcript']:
                auto_text = "ìë™" if video['transcript_auto'] else "ìˆ˜ë™"
                print(f"   ğŸ“ ìë§‰: âœ… ({video['transcript_language']}, {auto_text})")
            else:
                print(f"   ğŸ“ ìë§‰: âŒ")
            
            # ë¶„ì„ ì •ë³´
            if video['has_analysis']:
                print(f"   ğŸ¤– ë¶„ì„: âœ…")
                print(f"   ğŸ“ˆ ì¤‘ìš”ë„: {video['importance_score']:.2f} | ğŸ˜Š ê°ì •: {video['sentiment_score']:.2f}")
                print(f"   ğŸ’¬ ìš”ì•½: {video['summary'][:100]}...")
                
                if video['key_insights']:
                    print(f"   ğŸ’¡ ì¸ì‚¬ì´íŠ¸:")
                    for insight in video['key_insights'][:2]:
                        print(f"      â€¢ {insight}")
                
                if video['mentioned_entities']:
                    entities = ', '.join(video['mentioned_entities'][:5])
                    print(f"   ğŸ¢ ì£¼ìš” ì–¸ê¸‰: {entities}")
            else:
                print(f"   ğŸ¤– ë¶„ì„: âŒ")
            
            print(f"   ğŸ”— {video['video_url']}")
            print("-" * 50)
        
        # ëª¨ë‹ë¸Œë¦¬í”„ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ë¶„ì„
        if morning_brief_videos and analysis_count > 1:
            print(f"\nğŸ“Š ëª¨ë‹ë¸Œë¦¬í”„ ì¢…í•© ë¶„ì„")
            print("="*40)
            
            analyzed_morning = [v for v in analysis_results if v['has_analysis'] and 
                               any(keyword in v['video_title'].lower() for keyword in ['ëª¨ë‹ë¸Œë¦¬í”„', 'morning'])]
            
            if analyzed_morning:
                # ê°ì • ë³€í™” ì¶”ì´
                sentiments = [v['sentiment_score'] for v in reversed(analyzed_morning)]
                print(f"ğŸ“ˆ ê°ì • ì ìˆ˜ ë³€í™”: {' â†’ '.join([f'{s:.2f}' for s in sentiments])}")
                
                # ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆ
                most_important = max(analyzed_morning, key=lambda x: x['importance_score'])
                print(f"\nğŸ”¥ ê°€ì¥ ì¤‘ìš”í•œ ì´ìŠˆ:")
                print(f"   {most_important['video_title']}")
                print(f"   ì¤‘ìš”ë„: {most_important['importance_score']:.2f}")
                
                # ì£¼ìš” ì–¸ê¸‰ ì—”í‹°í‹° í†µê³„
                all_entities = []
                for v in analyzed_morning:
                    all_entities.extend(v['mentioned_entities'])
                
                entity_counts = {}
                for entity in all_entities:
                    entity_counts[entity] = entity_counts.get(entity, 0) + 1
                
                if entity_counts:
                    top_entities = sorted(entity_counts.items(), key=lambda x: x[1], reverse=True)[:5]
                    print(f"\nğŸ† ì£¼ê°„ í•« í‚¤ì›Œë“œ:")
                    for entity, count in top_entities:
                        print(f"   â€¢ {entity} ({count}íšŒ ì–¸ê¸‰)")
        
        return analysis_results
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None
    finally:
        db.close()

if __name__ == "__main__":
    analyze_existing_chesley() 